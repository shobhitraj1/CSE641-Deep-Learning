{"cells":[{"cell_type":"markdown","id":"76fdc4f4","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"c84110ebdb2fdf77fa35fd2b11d9f269","grade":false,"grade_id":"cell-1b9a733a18c86731","locked":true,"schema_version":3,"solution":false,"task":false},"id":"76fdc4f4"},"source":["# Assignment 2\n","## ✅ Rename the filename with your roll number. E.g. if your roll number is `MT24003` then rename the file `MT24003_a2.ipynb`.\n","## ✅ Write code only in the sections marked with `# YOUR CODE HERE`. No, you can NOT write code anywhere else.\n","## ✅ Download and extract the `data.zip` folder next to this file. If you extract it correctly, you will have a `data` folder next to this file.\n","## ✅ Submit a .zip (NOT .tar, .rar, etc) file containining:\n","###    1. This Notebook after filling the code where asked.\n","###    2. The loss and metric plots generated using the `save_training_report` functions [`auto_encoder.png` + `variational_auto_encoder.png` + `conditional_variational_auto_encoder.png`].\n","###    3. The model weights saved using the  `save_model_weights` functions [`auto_encoder.pth` + `variational_auto_encoder.pth` + `conditional_variational_auto_encoder.pth`].\n","## ❌ Do not modify any other function or class definitions; doing so may lead to the autograder failing to judge your submission, resulting in a zero.\n","## ❌ Deleting or adding new cells may lead to the `autograder` failing to judge your submission, resulting in a zero. Even if a cell is empty, do NOT delete it.\n","## ❌ Do NOT install / import any other libraries. You should be able to solve all the questions using only the libraries imported below."]},{"cell_type":"code","execution_count":1,"id":"20bad9ae","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"6d7b756fb6a23f890e8ca9761231ecce","grade":false,"grade_id":"cell-e5da815dd8a88fa1","locked":true,"schema_version":3,"solution":false,"task":false},"id":"20bad9ae","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738683182480,"user_tz":-330,"elapsed":275945,"user":{"displayName":"Aman Chauhan","userId":"07220292199924016605"}},"outputId":"7c49e2f1-595a-4516-9488-73e5b2e49efd"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 -q\n","!pip install numpy==1.25.2 -q\n","!pip install soundfile==0.13.0 -q\n","!pip install pandas==2.2.3 -q\n","!pip install matplotlib==3.9.4 -q\n","!pip install scikit-image==0.21.0 -q\n","!pip install tqdm==4.67.1 -q"]},{"cell_type":"code","execution_count":2,"id":"d72f04b9","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"7aaa74c34a4b7f5218e5e08b5e1ca9c4","grade":false,"grade_id":"cell-4d3f0796790fca27","locked":true,"schema_version":3,"solution":false,"task":false},"id":"d72f04b9","executionInfo":{"status":"ok","timestamp":1738683213721,"user_tz":-330,"elapsed":10454,"user":{"displayName":"Aman Chauhan","userId":"07220292199924016605"}}},"outputs":[],"source":["import os\n","import random\n","import timeit\n","from pathlib import Path\n","from typing import Tuple\n","from skimage.metrics import structural_similarity as ssim\n","from numpy import array as NumpyArray\n","from typing import List\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import torch\n","import torchvision\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"id":"1083fc2e","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"0d78a13c712a2219781883b29b72b8bb","grade":false,"grade_id":"cell-fbb5eaa8a23a48e4","locked":true,"schema_version":3,"solution":false,"task":false},"id":"1083fc2e"},"outputs":[],"source":["PATH_TO_DATA_DIR = Path(\"./data\")\n","PATH_TO_TRAIN_DATA_DIR = str(PATH_TO_DATA_DIR / \"train\")\n","PATH_TO_TEST_DATA_DIR = str(PATH_TO_DATA_DIR / \"test\")"]},{"cell_type":"markdown","id":"f9bf50f9","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"05c85f5742fd38ad1eece895f48e959d","grade":false,"grade_id":"cell-c71f318740837018","locked":true,"schema_version":3,"solution":false,"task":false},"id":"f9bf50f9"},"source":["# `q1`: `FashionMNIST` Dataset\n","1. Implement a Dataset class for the `FashionMNIST` data for the task of `Image Restoration`.\n","2. The task of `Image Restoration` is an [Ill-posed problem](https://en.wikipedia.org/wiki/Well-posed_problem) where the goal is to restore the original image from a corrupted image. Thus there may be more than one augmented image for each clean image, and vice versa.\n","3. The `data` directory has the following directory structure:\n","4. ```\n","\tdata\n","    ├── train\n","    │   ├── aug\n","    │   │   ├── <imagenumber>_<classlabel>.png\n","    │   │   ├── ...\n","    │   ├── clean\n","    │   │   ├── <imagenumber>_<classlabel>.png\n","    │   │   ├── ...\n","    └── test\n","        ├── aug\n","        │   ├── <imagenumber>_<classlabel>.png\n","        │   ├── ...\n","        └── clean\n","            ├── <imagenumber>_<classlabel>.png\n","            ├── ...\n","    ```\n","5. Constraints:\n","   1. The `__getitem__` method should return a tuple of the form `(aug_image, clean_image, label)`.  `clean_image` is the clean image, `aug_image` is the augmented image, and `label` is the class label of the image.\n","   2. Both `clean_image` and `aug_image` tensors should be of the shape `(1, 28, 28)` and of type `torch.float32`.\n","   3. Both `clean_image` and `aug_image` tensors should have pixel values between `[0, 1]`.\n","   4. `label` should be of type `torch.int64`.\n","\n","\n","`q1` Grading [Total: 1]: `1` point if the code runs without any errors on hidden test cases, otherwise `0` points. No partial points for this question."]},{"cell_type":"code","execution_count":null,"id":"f6774912","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"03537e5e11428ae611e2a1608711da6d","grade":false,"grade_id":"cell-8c0a42df86df9453","locked":false,"schema_version":3,"solution":true,"task":false},"id":"f6774912"},"outputs":[],"source":["class FashionMNISTDataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    A PyTorch Dataset for loading paired FashionMNIST images (augmented and clean versions).\n","\n","    Attributes:\n","        augmented_images (List[str]): List of file paths to augmented images, sorted alphabetically.\n","        clean_images (List[str]): List of file paths to clean images, sorted alphabetically.\n","    \"\"\"\n","    def __init__(\n","        self, path_to_augmented_images_dir: str, path_to_clean_images_dir: str\n","    ):\n","        \"\"\"\n","        Initializes the dataset by loading file paths for augmented and clean images.\n","\n","        Args:\n","            path_to_augmented_images_dir (str): Path to the directory containing augmented images.\n","            path_to_clean_images_dir (str): Path to the directory containing clean images.\n","        \"\"\"\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","\n","    def __len__(self) -> int:\n","        \"\"\"\n","        Returns the total number of samples in the dataset.\n","\n","        Returns:\n","            int\n","        \"\"\"\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","\n","    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor, int]:\n","        \"\"\"\n","        Retrieves the augmented image, clean image, and label for a given index.\n","\n","        Args:\n","            idx (int): Index of the sample to retrieve.\n","\n","        Returns:\n","            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","                - Augmented image as a tensor with values normalized to [0, 1].\n","                - Clean image as a tensor with values normalized to [0, 1].\n","                - Label as an integer tensor, extracted from the filename.\n","        \"\"\"\n","        # YOUR CODE HERE\n","        raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"id":"a15bfcd4","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"63ed2bc027f1a58e5d75e08c2441cd47","grade":true,"grade_id":"cell-3da55416cc383293","locked":true,"points":0.5,"schema_version":3,"solution":false,"task":false},"id":"a15bfcd4"},"outputs":[],"source":["# tests for q1\n","\n","path_to_train_images_aug_dir = str(PATH_TO_TRAIN_DATA_DIR + \"/aug\")\n","path_to_train_images_clean_dir = str(PATH_TO_TRAIN_DATA_DIR + \"/clean\")\n","fashion_mnist_dataset = FashionMNISTDataset(\n","    path_to_augmented_images_dir=path_to_train_images_aug_dir,\n","    path_to_clean_images_dir=path_to_train_images_clean_dir,\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"f10062cb","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"c2873f662ae24c8b90b1887ace7842f0","grade":true,"grade_id":"cell-f9471ff16e7f79bf","locked":true,"points":0.5,"schema_version":3,"solution":false,"task":false},"id":"f10062cb"},"outputs":[],"source":["# tests for q1\n","\n","path_to_test_images_aug_dir = str(PATH_TO_TEST_DATA_DIR + \"/aug\")\n","path_to_test_images_clean_dir = str(PATH_TO_TEST_DATA_DIR + \"/clean\")\n","fashion_mnist_dataset = FashionMNISTDataset(\n","    path_to_augmented_images_dir=path_to_test_images_aug_dir,\n","    path_to_clean_images_dir=path_to_test_images_clean_dir,\n",")\n","\n","\n","del fashion_mnist_dataset"]},{"cell_type":"markdown","id":"5f3a252f","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"409210f88f008c08b06665ec83565874","grade":false,"grade_id":"cell-465e4cf52baa821c","locked":true,"schema_version":3,"solution":false,"task":false},"id":"5f3a252f"},"source":["# `q2`: Encoder, and Decoder classes\n","\n","* Your task is to create AutoEncoder models for the task of `Image Restoration` using the `FashionMNIST` dataset. You need to implement the `Encoder` and `Decoder` classes for the AutoEncoder model. The `Encoder` class will be used to encode the input image into a latent representation, and the `Decoder` class will be used to decode the latent representation back to the original image. The `Encoder` and `Decoder` classes will be used in the AutoEncoder model, Variational AutoEncoder model, and (optionally) Conditional AutoEncoder model, so the implementation should be **generic and not specific** to any of the models.\n","* `q2a`: `Encoder` class: Implement a generic Encoder Module that will be used within all the AutoEncoder flavors (AutoEncoder, Variational AutoEncoder, and (optinally) Conditional AutoEncoder). Constraints:\n","  1. The input tensor will be of shape `[batch_size, 1, 28, 28]` that comes out of the DataLoader of the `FashionMNIST` dataset.\n","  2. Feel free to use any architecture you like with any layer or activation function in it. **You can NOT use pre-trained model weights**.\n","  3. The output tensor must be of shape `[batch_size, output_channels, height, width]`. This tensor will be the latent representation of the input tensor and will be passed to the Decoder Module.\n","  4. The number of parameters in the Encoder Module must be between 2,000 and 1,000,000 (both inclusive). Note that the number of parameters in the Encoder Module and Decoder Module will be counted separately and may not be the same.\n","\n","* `q2b`: `Decoder` class: Implement a generic Decoder Module that will be used within all the AutoEncoder flavors (AutoEncoder, Variational AutoEncoder, and (optinally) Conditional AutoEncoder). Constraints:\n","  1. The input tensor will be of shape `[batch_size, input_channels, height, width]` that comes out of the Encoder Module.\n","  2. Feel free to use any architecture you like with any layer or activation function in it. **You can NOT use pre-trained model weights**.\n","  3. The output tensor must be of shape `[batch_size, 1, 28, 28]`. This tensor will be the reconstructed image of the input tensor.\n","  4. The number of parameters in the Decoder Module must be between 2,000 and 1,000,000 (both inclusive). Note that the number of parameters in the Encoder Module and Decoder Module will be counted separately and may not be the same.\n","\n","`q2` Grading [Total: 1 point]:\n","1. `q2a`: `Encoder` class: `0.5` points if the code runs without any errors on hidden test cases, otherwise 0 points. No partial points for this question.\n","2. `q2b`: `Decoder` class:  `0.5` points if the code runs without any errors on hidden test cases, otherwise 0 points. No partial points for this question."]},{"cell_type":"markdown","id":"41dada3f","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"9a7c5d3320a9095bd8976ff6be7b3a45","grade":false,"grade_id":"cell-f32f37ee29097b08","locked":true,"schema_version":3,"solution":false,"task":false},"id":"41dada3f"},"source":["## `q2a`: `Encoder` class"]},{"cell_type":"code","execution_count":null,"id":"76a2ae32","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"eb55e5df0d2a384f0950d3dba1e569b0","grade":false,"grade_id":"cell-5213820ac6f88d24","locked":false,"schema_version":3,"solution":true,"task":false},"id":"76a2ae32"},"outputs":[],"source":["class Encoder(torch.nn.Module):\n","    def __init__(self, output_channels: int, type_of_autoencoder: str = None):\n","        super(Encoder, self).__init__()\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","\n","    def forward(self, x):\n","        # YOUR CODE HERE\n","        raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"id":"9c624622","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f980d449dc452dc8ce2a82edc236a735","grade":true,"grade_id":"cell-296f00226259527a","locked":true,"points":0.25,"schema_version":3,"solution":false,"task":false},"id":"9c624622"},"outputs":[],"source":["# tests for q2a\n","\n","encoder = Encoder(output_channels=64, type_of_autoencoder=\"vae\")\n","\n","random_input_tensor = torch.randn(1, 1, 28, 28)\n","output_tensor = encoder(random_input_tensor)\n","\n","\n","del encoder"]},{"cell_type":"markdown","id":"f17e534d","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"de375c0897a91a5a2154f5ce9386c72c","grade":false,"grade_id":"cell-ef1c74bb6101690f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"f17e534d"},"source":["## `q2b`: `Decoder` class"]},{"cell_type":"code","execution_count":null,"id":"451584ea","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"0fe4eb084b81a03bade98cb0f850ce07","grade":false,"grade_id":"cell-72c4cd996db5c348","locked":false,"schema_version":3,"solution":true,"task":false},"id":"451584ea"},"outputs":[],"source":["class Decoder(torch.nn.Module):\n","    def __init__(self, input_channels: int, type_of_autoencoder: str):\n","        super(Decoder, self).__init__()\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","\n","    def forward(self, x):\n","        # YOUR CODE HERE\n","        raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"id":"d25b4533","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"7d81ee273dd30e560f3bf8c9382097fd","grade":true,"grade_id":"cell-aba8bdf951c0b14d","locked":true,"points":0.25,"schema_version":3,"solution":false,"task":false},"id":"d25b4533"},"outputs":[],"source":["# tests for q2b\n","\n","decoder = Decoder(input_channels=64, type_of_autoencoder=\"vae\")\n","\n","random_input_tensor = torch.randn(1, 64, 4, 4)\n","output_tensor = decoder(random_input_tensor)\n","\n","\n","del decoder"]},{"cell_type":"markdown","id":"8da6b592","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"355eceafe54e9baa8373260d3a8f4d61","grade":false,"grade_id":"cell-d67cb659437ac2d8","locked":true,"schema_version":3,"solution":false,"task":false},"id":"8da6b592"},"source":["# `q3`: AutoEncoder Model\n","* `q3a`: `AutoEncoder` class: Implement a AutoEncoder that uses the Encoder and Decoder Modules implemented in `q2a` and `q2b`. Constraints:\n","  1. The number of parameters in the AutoEncoder must be between 4,000 and 2,000,000 (both inclusive).\n","  2. The input tensor will be of shape `[batch_size, 1, 28, 28]` that comes out of the DataLoader of the `FashionMNIST` dataset.\n","  3. The output tensor must be of shape `[batch_size, 1, 28, 28]`. This tensor will be the reconstructed image of the input tensor.\n","\n","* `q3b`: Training the models: Implement the training loop for the AutoEncoder model. Constraints:\n","  1. Use the `FashionMNIST` dataset implemented in `q1` to load the data.\n","  2. Use the `AutoEncoder` model implemented in `q3a`.\n","  3. You are free to choose any loss function, optimizer, and hyperparameters.\n","  4. **You must**:\n","     1. Book-keep the training and validation losses and SSIM scores for each epoch and use it to plot the training curves with the `AutoEncoder.save_training_report` method.\n","     2. To calculate the SSIM score, you can use the `get_ssim` function provided below.\n","     3. Save the model weights using `AutoEncoder.save_model_weights` method.\n","\n","\n","`q3` Grading [Total: 1.5 points]:\n","1. `q3a`: `AutoEncoder` class: `0.5` points if the code runs without any errors on hidden test cases, otherwise 0 points. No partial points for this question.\n","2. `q3b`: Training the models: `1` points. You will be awarded points based on the SSIM score of the `AutoEncoder` model on a **hidden test set**. The grading will be as follows:\n","   1. 0.8 or more: `1` point\n","   2. 0.7 to 0.79: `0.8` points\n","   3. 0.6 to 0.69: `0.6` points\n","   4. 0.5 to 0.59: `0.4` points\n","   5. 0.4 to 0.49: `0.2` points\n","   6. Less than 0.4: `0` points\n","\n","\n","You are provided with the following template. **Populate only the sections marked as `# YOUR CODE HERE`. Do not modify other parts of the template.**"]},{"cell_type":"markdown","id":"d851bba2","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"cdb87617aba0d4fb4b04e357611f17f3","grade":false,"grade_id":"cell-3f5327832830d42f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"d851bba2"},"source":["## `q3a`: `AutoEncoder` class"]},{"cell_type":"code","execution_count":null,"id":"7d4820e2","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"6295f3b83dd4783e08e7b66f2f7e03ea","grade":false,"grade_id":"cell-c095dffe3237e062","locked":false,"schema_version":3,"solution":true,"task":false},"id":"7d4820e2"},"outputs":[],"source":["class AutoEncoder(torch.nn.Module):\n","    def __init__(self, latent_dim: int):\n","        super(AutoEncoder, self).__init__()\n","        self.encoder = Encoder(output_channels=latent_dim, type_of_autoencoder=\"ae\")\n","        self.decoder = Decoder(input_channels=latent_dim, type_of_autoencoder=\"ae\")\n","        self.latent_dim = latent_dim\n","\n","    def forward(self, input_tensor):\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","\n","    def save_model_weights(self):\n","        torch.save(self.state_dict(), \"auto_encoder.pth\")\n","\n","    def load_model_weights(self):\n","        self.load_state_dict(torch.load(\"auto_encoder.pth\"))\n","\n","    def save_training_report(\n","        self,\n","        list_of_train_losses: List[float],\n","        list_of_val_losses: List[float],\n","        list_of_train_ssim_scores: List[float],\n","        list_of_val_ssim_scores: List[float],\n","    ):\n","        plt.figure(figsize=(10, 5))\n","\n","        plt.subplot(1, 2, 1)\n","        plt.title(\"Loss per Epoch\")\n","        plt.plot(list_of_train_losses, label=\"Training\")\n","        plt.plot(list_of_val_losses, label=\"Validation\")\n","        plt.xlabel(\"Epoch\")\n","        plt.ylabel(\"Loss\")\n","        plt.legend()\n","\n","        plt.subplot(1, 2, 2)\n","        plt.title(\"SSIM per Epoch\")\n","        plt.plot(list_of_train_ssim_scores, label=\"Training\")\n","        plt.plot(list_of_val_ssim_scores, label=\"Validation\")\n","        plt.xlabel(\"Epoch\")\n","        plt.ylabel(\"SSIM\")\n","        plt.legend()\n","\n","        plt.suptitle(\"AutoEncoder Training Report\")\n","\n","        plt.savefig(\"auto_encoder.png\")\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"id":"e9886dc4","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"70b93c299aaf9f281134acf8f90ec88b","grade":true,"grade_id":"cell-a3e42ddd163c3984","locked":true,"points":0.5,"schema_version":3,"solution":false,"task":false},"id":"e9886dc4"},"outputs":[],"source":["# tests for q3a\n","\n","autoencoder = AutoEncoder(latent_dim=64)\n","\n","random_input_tensor = torch.randn(1, 1, 28, 28)\n","output_tensor = autoencoder(random_input_tensor)\n","\n","\n","del autoencoder"]},{"cell_type":"markdown","id":"64f21c54","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"0c7b5a2136e3dcd1216b7e542d3ff16d","grade":false,"grade_id":"cell-65078b6416cca373","locked":true,"schema_version":3,"solution":false,"task":false},"id":"64f21c54"},"source":["## `q3b`: Training the model"]},{"cell_type":"code","execution_count":null,"id":"63461f1a","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"1ceb34aa91fc9468cb1272d8233d2f8c","grade":false,"grade_id":"cell-38c2e638d5b531c3","locked":true,"schema_version":3,"solution":false,"task":false},"id":"63461f1a"},"outputs":[],"source":["def get_ssim(\n","    list_of_predicted_images: List[NumpyArray], list_of_true_images: List[NumpyArray]\n",") -> List[float]:\n","    ssim_values = []\n","    for predicted_image, true_image in zip(\n","        list_of_predicted_images, list_of_true_images\n","    ):\n","        assert predicted_image.shape == (\n","            28,\n","            28,\n","        ), f\"Expected image of shape (28, 28) but got {predicted_image.shape}\"\n","        assert true_image.shape == (\n","            28,\n","            28,\n","        ), f\"Expected image of shape (28, 28) but got {true_image.shape}\"\n","        ssim_values.append(\n","            ssim(\n","                predicted_image,\n","                true_image,\n","                data_range=true_image.max() - true_image.min(),\n","            )\n","        )\n","    return sum(ssim_values) / len(ssim_values)"]},{"cell_type":"code","execution_count":null,"id":"f032eb85","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"3f727e24e154cd61dba5dcfda0712500","grade":false,"grade_id":"cell-7a9e6fc60429fdec","locked":false,"schema_version":3,"solution":true,"task":false},"id":"f032eb85"},"outputs":[],"source":["# Use this cell to:\n","# 1. Train the AutoEncoder model while bookkeeping the training and validation losses and SSIM scores for each epoch\n","# 2. Save the model weights using AutoEncoder.save_model_weights method\n","# 3. Save the training report using AutoEncoder.save_training_report method\n","\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"id":"8253ac7b-3a7b-4cc9-a275-f07641d0053f","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"6a42141696fc0c895626537a50aba0e1","grade":true,"grade_id":"cell-a2e4e8700fdcfb80","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"8253ac7b-3a7b-4cc9-a275-f07641d0053f"},"outputs":[],"source":["# tests for q3b"]},{"cell_type":"markdown","id":"b6e8ac3e","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"382a5e2833d97007f5c3e1b07000a523","grade":false,"grade_id":"cell-8f7b007d1bcbf8e1","locked":true,"schema_version":3,"solution":false,"task":false},"id":"b6e8ac3e"},"source":["# `q4`: Variational AutoEncoder Model\n","* `q4a`: `VariationalAutoEncoder` class: Implement a VariationalAutoEncoder that uses the Encoder and Decoder Modules implemented in `q2a` and `q2b`. Constraints:\n","  1. The number of parameters in the VariationalAutoEncoder must be between 4,000 and 2,000,000 (both inclusive).\n","  2. The input tensor will be of shape `[batch_size, 1, 28, 28]` that comes out of the DataLoader of the `FashionMNIST` dataset.\n","  3. The output tensor must be of shape `[batch_size, 1, 28, 28]`. This tensor will be the reconstructed image of the input tensor.\n","\n","* `q4b`: Training the models: Implement the training loop for the VariationalAutoEncoder model. Constraints:\n","  1. Use the `FashionMNIST` dataset implemented in `q1` to load the data.\n","  2. Use the `VariationalAutoEncoder` model implemented in `q4a`.\n","  3. You are free to choose any loss function, optimizer, and hyperparameters.\n","  4. **You must**:\n","     1. Book-keep the training and validation losses and SSIM scores for each epoch and use it to plot the training curves with the `VariationalAutoEncoder.save_training_report` method.\n","     2. To calculate the SSIM score, you can use the `get_ssim` function provided below.\n","     3. Save the model weights using `VariationalAutoEncoder.save_model_weights` method.\n","\n","\n","`q4` Grading [Total: 1.5 points]:\n","1. `q4a`: `VariationalAutoEncoder` class: `0.5` points if the code runs without any errors on hidden test cases, otherwise 0 points. No partial points for this question.\n","2. `q4b`: Training the models: `1` points. You will be awarded points based on the SSIM score of the `VariationalAutoEncoder` model on a **hidden test set**. The grading will be as follows:\n","   1. 0.8 or more: `1` point\n","   2. 0.7 or more: `0.8` points\n","   3. 0.6 or more: `0.6` points\n","   4. 0.5 or more: `0.4` points\n","   5. 0.4 or more: `0.2` points\n","   6. Less than 0.4: `0` points\n","\n","You are provided with the following template. **Populate only the sections marked as `# YOUR CODE HERE`. Do not modify other parts of the template.**"]},{"cell_type":"markdown","id":"67530b04","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"66752f0b784a02c127ff8a044608eb65","grade":false,"grade_id":"cell-897af261c2d06d30","locked":true,"schema_version":3,"solution":false,"task":false},"id":"67530b04"},"source":["## `q4a`: `VariationalAutoEncoder` class"]},{"cell_type":"code","execution_count":null,"id":"9ef0f31c","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"bce3eede85dba7a8fb5be994467d265c","grade":false,"grade_id":"cell-50018b56d6158ef5","locked":false,"schema_version":3,"solution":true,"task":false},"id":"9ef0f31c"},"outputs":[],"source":["class VariationalAutoEncoder(torch.nn.Module):\n","    def __init__(self, latent_dim: int):\n","        super(VariationalAutoEncoder, self).__init__()\n","        self.encoder = Encoder(output_channels=latent_dim * 2, type_of_autoencoder=\"vae\")\n","        self.decoder = Decoder(input_channels=latent_dim, type_of_autoencoder=\"vae\")\n","        self.latent_dim = latent_dim\n","\n","    def reparameterize(self, mu, log_var):\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","\n","    def forward(self, input_tensor):\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","\n","    def loss_function(self, predicted_images, gt_images, mu, log_var):\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","\n","    def save_model_weights(self):\n","        torch.save(self.state_dict(), \"variational_auto_encoder.pth\")\n","\n","    def load_model_weights(self):\n","        self.load_state_dict(torch.load(\"variational_auto_encoder.pth\"))\n","\n","    def save_training_report(\n","        self,\n","        list_of_train_losses: List[float],\n","        list_of_val_losses: List[float],\n","        list_of_train_ssim_scores: List[float],\n","        list_of_val_ssim_scores: List[float],\n","    ):\n","        plt.figure(figsize=(10, 5))\n","\n","        plt.subplot(1, 2, 1)\n","        plt.title(\"Loss per Epoch\")\n","        plt.plot(list_of_train_losses, label=\"Training\")\n","        plt.plot(list_of_val_losses, label=\"Validation\")\n","        plt.xlabel(\"Epoch\")\n","        plt.ylabel(\"Loss\")\n","        plt.legend()\n","\n","        plt.subplot(1, 2, 2)\n","        plt.title(\"SSIM per Epoch\")\n","        plt.plot(list_of_train_ssim_scores, label=\"Training\")\n","        plt.plot(list_of_val_ssim_scores, label=\"Validation\")\n","        plt.xlabel(\"Epoch\")\n","        plt.ylabel(\"SSIM\")\n","        plt.legend()\n","\n","        plt.suptitle(\"VariationalAutoEncoder Training Report\")\n","\n","        plt.savefig(\"variational_auto_encoder.png\")\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"id":"8e19e4e3","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"9743b03fdaabcc90c9b291e4bc45b4e2","grade":true,"grade_id":"cell-fb81ba17291dc3b0","locked":true,"points":0.5,"schema_version":3,"solution":false,"task":false},"id":"8e19e4e3"},"outputs":[],"source":["# tests for q4a\n","\n","variational_autoencoder = VariationalAutoEncoder(latent_dim=64)\n","\n","random_input_tensor = torch.randn(1, 1, 28, 28)\n","output = variational_autoencoder(random_input_tensor)\n","\n","\n","del variational_autoencoder"]},{"cell_type":"markdown","id":"7c56ef92","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"26dc99239a593c1549c64375c7d685ab","grade":false,"grade_id":"cell-9d6349d66274c9ce","locked":true,"schema_version":3,"solution":false,"task":false},"id":"7c56ef92"},"source":["## `q4b`: Training the model"]},{"cell_type":"code","execution_count":null,"id":"15f88a4c","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"6c621752f2b40a4b066d930c0b1ff621","grade":false,"grade_id":"cell-0aea62ae6e9aaf70","locked":false,"schema_version":3,"solution":true,"task":false},"id":"15f88a4c"},"outputs":[],"source":["# Use this cell to:\n","# 1. Train the VariationalAutoEncoder model while bookkeeping the training and validation losses and SSIM scores for each epoch\n","# 2. Save the model weights using VariationalAutoEncoder.save_model_weights method\n","# 3. Save the training report using VariationalAutoEncoder.save_training_report method\n","\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"id":"e756b79e-a3bc-4fed-868d-9229608db4c1","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"dc7ca536b2223b235453d4c5de1a9f02","grade":true,"grade_id":"cell-7784e404fa72538a","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"e756b79e-a3bc-4fed-868d-9229608db4c1"},"outputs":[],"source":["# tests for q4b"]},{"cell_type":"markdown","id":"2a2cf5a7","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"fc470904174ea8ba369214a879982556","grade":false,"grade_id":"cell-8ff254023bae8425","locked":true,"schema_version":3,"solution":false,"task":false},"id":"2a2cf5a7"},"source":["# `q5`: [BONUS] Conditional Variational AutoEncoder Model\n","* `q5a`: `ConditionalVariationalAutoEncoder` class: Implement a ConditionalVariationalAutoEncoder that uses the Encoder and Decoder Modules implemented in `q2a` and `q2b`. Constraints:\n","  1. The number of parameters in the ConditionalVariationalAutoEncoder must be between 4,000 and 2,000,000 (both inclusive).\n","  2. The input tensor will be of shape `[batch_size, 1, 28, 28]` that comes out of the DataLoader of the `FashionMNIST` dataset.\n","  3. The output tensor must be of shape `[batch_size, 1, 28, 28]`. This tensor will be the reconstructed image of the input tensor.\n","\n","* `q5b`: Training the models: Implement the training loop for the ConditionalVariationalAutoEncoder model. Constraints:\n","  1. Use the `FashionMNIST` dataset implemented in `q1` to load the data.\n","  2. Use the `ConditionalVariationalAutoEncoder` model implemented in `q5a`.\n","  3. You are free to choose any loss function, optimizer, and hyperparameters.\n","  4. **You must**:\n","     1. Book-keep the training and validation losses and SSIM scores for each epoch and use it to plot the training curves with the `ConditionalVariationalAutoEncoder.save_training_report` method.\n","     2. To calculate the SSIM score, you can use the `get_ssim` function provided below.\n","     3. Save the model weights using `ConditionalVariationalAutoEncoder.save_model_weights` method.\n","\n","\n","`q5` Grading [Total: 1 point]:\n","1. `q5b`: Training the models: `1` point. You will be awarded points based on the SSIM score of the `ConditionalVariationalAutoEncoder` model on a **hidden test set**. The grading will be as follows:\n","   1. 0.8 or more: `1` point\n","   2. 0.7 to 0.79: `0.5` points\n","   3. less than 0.7: `0` points\n","\n","You are provided with the following template. **Populate only the sections marked as `# YOUR CODE HERE`. Do not modify other parts of the template.**"]},{"cell_type":"markdown","id":"6bfd3d62","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"8027ac7c9c037cb063eb577db3b98dcb","grade":false,"grade_id":"cell-219f459034a85820","locked":true,"schema_version":3,"solution":false,"task":false},"id":"6bfd3d62"},"source":["## `q5a`: [BONUS]`ConditionalVariationalAutoEncoder`"]},{"cell_type":"code","execution_count":null,"id":"29d7b335","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"46202c4d0e0385e99644013adea0cc20","grade":false,"grade_id":"cell-897bbc51499b7fd5","locked":false,"schema_version":3,"solution":true,"task":false},"id":"29d7b335"},"outputs":[],"source":["class ConditionalVariationalAutoEncoder(torch.nn.Module):\n","    def __init__(self, latent_dim: int, condition_dim: int):\n","        super(ConditionalVariationalAutoEncoder, self).__init__()\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","\n","    def reparameterize(self, mu, log_var):\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","\n","    def forward(self, input_tensor, condition_tensor):\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","\n","    def loss_function(self, predicted_images, gt_images, mu, log_var):\n","        # YOUR CODE HERE\n","        raise NotImplementedError()\n","\n","    def save_model_weights(self):\n","        torch.save(self.state_dict(), \"conditional_variational_auto_encoder.pth\")\n","\n","    def load_model_weights(self):\n","        self.load_state_dict(torch.load(\"conditional_variational_auto_encoder.pth\"))\n","\n","    def save_training_report(\n","        self,\n","        list_of_train_losses: List[float],\n","        list_of_val_losses: List[float],\n","        list_of_train_ssim_scores: List[float],\n","        list_of_val_ssim_scores: List[float],\n","    ):\n","        plt.figure(figsize=(10, 5))\n","\n","        plt.subplot(1, 2, 1)\n","        plt.title(\"Loss per Epoch\")\n","        plt.plot(list_of_train_losses, label=\"Training\")\n","        plt.plot(list_of_val_losses, label=\"Validation\")\n","        plt.xlabel(\"Epoch\")\n","        plt.ylabel(\"Loss\")\n","        plt.legend()\n","\n","        plt.subplot(1, 2, 2)\n","        plt.title(\"SSIM per Epoch\")\n","        plt.plot(list_of_train_ssim_scores, label=\"Training\")\n","        plt.plot(list_of_val_ssim_scores, label=\"Validation\")\n","        plt.xlabel(\"Epoch\")\n","        plt.ylabel(\"SSIM\")\n","        plt.legend()\n","\n","        plt.suptitle(\"ConditionalVariationalAutoEncoder Training Report\")\n","\n","        plt.savefig(\"conditional_variational_auto_encoder.png\")\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"id":"047b58dc","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"a1a03cab93a7876fc0502e6503bd4e6f","grade":true,"grade_id":"cell-53fef8fb01242647","locked":true,"points":0.5,"schema_version":3,"solution":false,"task":false},"id":"047b58dc"},"outputs":[],"source":["# tests for q5a\n","\n","conditional_variational_autoencoder = ConditionalVariationalAutoEncoder(\n","    latent_dim=64, condition_dim=10\n",")\n","\n","random_input_tensor = torch.randn(1, 1, 28, 28)\n","random_condition_tensor = torch.randn(1, 10)\n","output = conditional_variational_autoencoder(\n","    random_input_tensor, random_condition_tensor\n",")\n","\n","\n","del conditional_variational_autoencoder"]},{"cell_type":"markdown","id":"479811ee","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"15c443b0bed37da59019887932b70ac9","grade":false,"grade_id":"cell-5f32276d86af0895","locked":true,"schema_version":3,"solution":false,"task":false},"id":"479811ee"},"source":["## `q5b`: [BONUS] Training the model"]},{"cell_type":"code","execution_count":null,"id":"3e38dc58","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"3beaab2f82c0121782c2bd7506375a93","grade":false,"grade_id":"cell-93d3753839989066","locked":false,"schema_version":3,"solution":true,"task":false},"id":"3e38dc58"},"outputs":[],"source":["# Use this cell to:\n","# 1. Train the ConditionalVariationalAutoEncoder model while bookkeeping the training and validation losses and SSIM scores for each epoch\n","# 2. Save the model weights using ConditionalVariationalAutoEncoder.save_model_weights method\n","# 3. Save the training report using ConditionalVariationalAutoEncoder.save_training_report method\n","\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"id":"5b6adadf-85a4-4bc3-b515-16e91bbe5193","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"4644e975dc1db68368a0a14613d8c03b","grade":true,"grade_id":"cell-13c675af9381345f","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"5b6adadf-85a4-4bc3-b515-16e91bbe5193"},"outputs":[],"source":["# tests for q5b"]},{"cell_type":"code","execution_count":null,"id":"558d12e1-5bdc-40ad-87c9-135ce0c553ae","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"e5a387ee031cf135774640a6a5c2c535","grade":true,"grade_id":"cell-1b8718af75c8a798","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"558d12e1-5bdc-40ad-87c9-135ce0c553ae"},"outputs":[],"source":["# tests for q3b, q4b, q5b\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.21"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}