{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76fdc4f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "76fdc4f4",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c84110ebdb2fdf77fa35fd2b11d9f269",
     "grade": false,
     "grade_id": "cell-1b9a733a18c86731",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 2\n",
    "## ✅ Rename the filename with your roll number. E.g. if your roll number is `MT24003` then rename the file `MT24003_a2.ipynb`.\n",
    "## ✅ Write code only in the sections marked with `# YOUR CODE HERE`. No, you can NOT write code anywhere else.\n",
    "## ✅ Download and extract the `data.zip` folder next to this file. If you extract it correctly, you will have a `data` folder next to this file.\n",
    "## ✅ Submit a .zip (NOT .tar, .rar, etc) file containining:\n",
    "###    1. This Notebook after filling the code where asked.\n",
    "###    2. The loss and metric plots generated using the `save_training_report` functions [`auto_encoder.png` + `variational_auto_encoder.png` + `conditional_variational_auto_encoder.png`].\n",
    "###    3. The model weights saved using the  `save_model_weights` functions [`auto_encoder.pth` + `variational_auto_encoder.pth` + `conditional_variational_auto_encoder.pth`].\n",
    "## ❌ Do not modify any other function or class definitions; doing so may lead to the autograder failing to judge your submission, resulting in a zero.\n",
    "## ❌ Deleting or adding new cells may lead to the `autograder` failing to judge your submission, resulting in a zero. Even if a cell is empty, do NOT delete it.\n",
    "## ❌ Do NOT install / import any other libraries. You should be able to solve all the questions using only the libraries imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bad9ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 275945,
     "status": "ok",
     "timestamp": 1738683182480,
     "user": {
      "displayName": "Aman Chauhan",
      "userId": "07220292199924016605"
     },
     "user_tz": -330
    },
    "id": "20bad9ae",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d7b756fb6a23f890e8ca9761231ecce",
     "grade": false,
     "grade_id": "cell-e5da815dd8a88fa1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "7c49e2f1-595a-4516-9488-73e5b2e49efd"
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 -q\n",
    "!pip install numpy==1.25.2 -q\n",
    "!pip install soundfile==0.13.0 -q\n",
    "!pip install pandas==2.2.3 -q\n",
    "!pip install matplotlib==3.9.4 -q\n",
    "!pip install scikit-image==0.21.0 -q\n",
    "!pip install tqdm==4.67.1 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72f04b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 10454,
     "status": "ok",
     "timestamp": 1738683213721,
     "user": {
      "displayName": "Aman Chauhan",
      "userId": "07220292199924016605"
     },
     "user_tz": -330
    },
    "id": "d72f04b9",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7aaa74c34a4b7f5218e5e08b5e1ca9c4",
     "grade": false,
     "grade_id": "cell-4d3f0796790fca27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import timeit\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from numpy import array as NumpyArray\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1083fc2e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1083fc2e",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d78a13c712a2219781883b29b72b8bb",
     "grade": false,
     "grade_id": "cell-fbb5eaa8a23a48e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA_DIR = Path(\"./data\")\n",
    "PATH_TO_TRAIN_DATA_DIR = str(PATH_TO_DATA_DIR / \"train\")\n",
    "PATH_TO_TEST_DATA_DIR = str(PATH_TO_DATA_DIR / \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bf50f9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "f9bf50f9",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05c85f5742fd38ad1eece895f48e959d",
     "grade": false,
     "grade_id": "cell-c71f318740837018",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# `q1`: `FashionMNIST` Dataset\n",
    "1. Implement a Dataset class for the `FashionMNIST` data for the task of `Image Restoration`.\n",
    "2. The task of `Image Restoration` is an [Ill-posed problem](https://en.wikipedia.org/wiki/Well-posed_problem) where the goal is to restore the original image from a corrupted image. Thus there may be more than one augmented image for each clean image, and vice versa.\n",
    "3. The `data` directory has the following directory structure:\n",
    "4. ```\n",
    "\tdata\n",
    "    ├── train\n",
    "    │   ├── aug\n",
    "    │   │   ├── <imagenumber>_<classlabel>.png\n",
    "    │   │   ├── ...\n",
    "    │   ├── clean\n",
    "    │   │   ├── <imagenumber>_<classlabel>.png\n",
    "    │   │   ├── ...\n",
    "    └── test\n",
    "        ├── aug\n",
    "        │   ├── <imagenumber>_<classlabel>.png\n",
    "        │   ├── ...\n",
    "        └── clean\n",
    "            ├── <imagenumber>_<classlabel>.png\n",
    "            ├── ...\n",
    "    ```\n",
    "5. Constraints:\n",
    "   1. The `__getitem__` method should return a tuple of the form `(aug_image, clean_image, label)`.  `clean_image` is the clean image, `aug_image` is the augmented image, and `label` is the class label of the image.\n",
    "   2. Both `clean_image` and `aug_image` tensors should be of the shape `(1, 28, 28)` and of type `torch.float32`.\n",
    "   3. Both `clean_image` and `aug_image` tensors should have pixel values between `[0, 1]`.\n",
    "   4. `label` should be of type `torch.int64`.\n",
    "\n",
    "\n",
    "`q1` Grading [Total: 1]: `1` point if the code runs without any errors on hidden test cases, otherwise `0` points. No partial points for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6774912",
   "metadata": {
    "deletable": false,
    "id": "f6774912",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03537e5e11428ae611e2a1608711da6d",
     "grade": false,
     "grade_id": "cell-8c0a42df86df9453",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class FashionMNISTDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for loading paired FashionMNIST images (augmented and clean versions).\n",
    "\n",
    "    Attributes:\n",
    "        augmented_images (List[str]): List of file paths to augmented images, sorted alphabetically.\n",
    "        clean_images (List[str]): List of file paths to clean images, sorted alphabetically.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, path_to_augmented_images_dir: str, path_to_clean_images_dir: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading file paths for augmented and clean images.\n",
    "\n",
    "        Args:\n",
    "            path_to_augmented_images_dir (str): Path to the directory containing augmented images.\n",
    "            path_to_clean_images_dir (str): Path to the directory containing clean images.\n",
    "        \"\"\"\n",
    "\n",
    "        clean_files = os.listdir(path_to_clean_images_dir)\n",
    "        augmented_files = os.listdir(path_to_augmented_images_dir)\n",
    "\n",
    "        # print(clean_files[:5])\n",
    "        # print(augmented_files[:5])\n",
    "        # print(f'Number of clean files is {len(clean_files)}')\n",
    "        # print(f'Number of augmented files is {len(augmented_files)}')\n",
    "\n",
    "        self.clean_images = [os.path.join(path_to_clean_images_dir, file) for file in clean_files]\n",
    "        self.augmented_images = [os.path.join(path_to_augmented_images_dir, file) for file in augmented_files]\n",
    "\n",
    "        self.clean_images.sort()\n",
    "        self.augmented_images.sort()\n",
    "\n",
    "        # print(f'First 5 clean images are {self.clean_images[:5]}')\n",
    "        # print(f'First 5 augmented images are {self.augmented_images[:5]}')\n",
    "        # print(f'Number of clean images is {len(self.clean_images)}')\n",
    "        # print(f'Number of augmented images is {len(self.augmented_images)}')\n",
    "\n",
    "        self.clean_images_images = [torchvision.io.read_image(image).float() for image in self.clean_images]\n",
    "        self.augmented_images_images = [torchvision.io.read_image(image).float() for image in self.augmented_images]\n",
    "\n",
    "        # normalizing the images\n",
    "        self.clean_images_images = [image / 255.0 for image in self.clean_images_images]\n",
    "        self.augmented_images_images = [image / 255.0 for image in self.augmented_images_images]\n",
    "\n",
    "        self.labels = [int(image.split(\"/\")[-1].split(\"_\")[1][0]) for image in self.clean_images]\n",
    "        self.labels = [torch.tensor(label, dtype=torch.int64) for label in self.labels]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Retrieves the augmented image, clean image, and label for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                - Augmented image as a tensor with values normalized to [0, 1].\n",
    "                - Clean image as a tensor with values normalized to [0, 1].\n",
    "                - Label as an integer tensor, extracted from the filename.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.augmented_images_images[idx], self.clean_images_images[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15bfcd4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "a15bfcd4",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63ed2bc027f1a58e5d75e08c2441cd47",
     "grade": true,
     "grade_id": "cell-3da55416cc383293",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests for q1\n",
    "\n",
    "path_to_train_images_aug_dir = str(PATH_TO_TRAIN_DATA_DIR + \"/aug\")\n",
    "path_to_train_images_clean_dir = str(PATH_TO_TRAIN_DATA_DIR + \"/clean\")\n",
    "fashion_mnist_dataset = FashionMNISTDataset(\n",
    "    path_to_augmented_images_dir=path_to_train_images_aug_dir,\n",
    "    path_to_clean_images_dir=path_to_train_images_clean_dir,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10062cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "f10062cb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2873f662ae24c8b90b1887ace7842f0",
     "grade": true,
     "grade_id": "cell-f9471ff16e7f79bf",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests for q1\n",
    "\n",
    "path_to_test_images_aug_dir = str(PATH_TO_TEST_DATA_DIR + \"/aug\")\n",
    "path_to_test_images_clean_dir = str(PATH_TO_TEST_DATA_DIR + \"/clean\")\n",
    "fashion_mnist_dataset = FashionMNISTDataset(\n",
    "    path_to_augmented_images_dir=path_to_test_images_aug_dir,\n",
    "    path_to_clean_images_dir=path_to_test_images_clean_dir,\n",
    ")\n",
    "\n",
    "\n",
    "del fashion_mnist_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a252f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5f3a252f",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "409210f88f008c08b06665ec83565874",
     "grade": false,
     "grade_id": "cell-465e4cf52baa821c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# `q2`: Encoder, and Decoder classes\n",
    "\n",
    "* Your task is to create AutoEncoder models for the task of `Image Restoration` using the `FashionMNIST` dataset. You need to implement the `Encoder` and `Decoder` classes for the AutoEncoder model. The `Encoder` class will be used to encode the input image into a latent representation, and the `Decoder` class will be used to decode the latent representation back to the original image. The `Encoder` and `Decoder` classes will be used in the AutoEncoder model, Variational AutoEncoder model, and (optionally) Conditional AutoEncoder model, so the implementation should be **generic and not specific** to any of the models.\n",
    "* `q2a`: `Encoder` class: Implement a generic Encoder Module that will be used within all the AutoEncoder flavors (AutoEncoder, Variational AutoEncoder, and (optinally) Conditional AutoEncoder). Constraints:\n",
    "  1. The input tensor will be of shape `[batch_size, 1, 28, 28]` that comes out of the DataLoader of the `FashionMNIST` dataset.\n",
    "  2. Feel free to use any architecture you like with any layer or activation function in it. **You can NOT use pre-trained model weights**.\n",
    "  3. The output tensor must be of shape `[batch_size, output_channels, height, width]`. This tensor will be the latent representation of the input tensor and will be passed to the Decoder Module.\n",
    "  4. The number of parameters in the Encoder Module must be between 2,000 and 1,000,000 (both inclusive). Note that the number of parameters in the Encoder Module and Decoder Module will be counted separately and may not be the same.\n",
    "\n",
    "* `q2b`: `Decoder` class: Implement a generic Decoder Module that will be used within all the AutoEncoder flavors (AutoEncoder, Variational AutoEncoder, and (optinally) Conditional AutoEncoder). Constraints:\n",
    "  1. The input tensor will be of shape `[batch_size, input_channels, height, width]` that comes out of the Encoder Module.\n",
    "  2. Feel free to use any architecture you like with any layer or activation function in it. **You can NOT use pre-trained model weights**.\n",
    "  3. The output tensor must be of shape `[batch_size, 1, 28, 28]`. This tensor will be the reconstructed image of the input tensor.\n",
    "  4. The number of parameters in the Decoder Module must be between 2,000 and 1,000,000 (both inclusive). Note that the number of parameters in the Encoder Module and Decoder Module will be counted separately and may not be the same.\n",
    "\n",
    "`q2` Grading [Total: 1 point]:\n",
    "1. `q2a`: `Encoder` class: `0.5` points if the code runs without any errors on hidden test cases, otherwise 0 points. No partial points for this question.\n",
    "2. `q2b`: `Decoder` class:  `0.5` points if the code runs without any errors on hidden test cases, otherwise 0 points. No partial points for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dada3f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "41dada3f",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a7c5d3320a9095bd8976ff6be7b3a45",
     "grade": false,
     "grade_id": "cell-f32f37ee29097b08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `q2a`: `Encoder` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a2ae32",
   "metadata": {
    "deletable": false,
    "id": "76a2ae32",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb55e5df0d2a384f0950d3dba1e569b0",
     "grade": false,
     "grade_id": "cell-5213820ac6f88d24",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, output_channels: int, type_of_autoencoder: str = None):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        torch.manual_seed(42)\n",
    "        self.type_of_autoencoder = type_of_autoencoder\n",
    "\n",
    "        class ResidualBlock(torch.nn.Module):\n",
    "            def __init__(self, in_channels: int):\n",
    "                super(ResidualBlock, self).__init__()\n",
    "\n",
    "                torch.manual_seed(42)\n",
    "                self.residual_block = torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "                    torch.nn.InstanceNorm2d(in_channels),\n",
    "                    torch.nn.LeakyReLU(negative_slope=0.1),\n",
    "                    torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "                    torch.nn.InstanceNorm2d(in_channels)\n",
    "                )\n",
    "\n",
    "            def forward(self, x):\n",
    "                return x + self.residual_block(x) \n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, 3, stride=2, padding=1),\n",
    "            torch.nn.InstanceNorm2d(32),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            ResidualBlock(32),\n",
    "            torch.nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            torch.nn.InstanceNorm2d(64),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            ResidualBlock(64),\n",
    "            torch.nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            torch.nn.InstanceNorm2d(128),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            ResidualBlock(128),\n",
    "            torch.nn.Conv2d(128, output_channels, 3, stride=1, padding=1),\n",
    "            torch.nn.InstanceNorm2d(output_channels),\n",
    "            torch.nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        # num_params = sum(p.numel() for p in self.parameters())\n",
    "        # print(f\"Encoder parameters: {num_params}\")\n",
    "\n",
    "        # reference: https://datascience.stackexchange.com/questions/49709/what-is-the-best-architecture-for-auto-encoder-for-image-reconstruction\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c624622",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9c624622",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f980d449dc452dc8ce2a82edc236a735",
     "grade": true,
     "grade_id": "cell-296f00226259527a",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests for q2a\n",
    "\n",
    "encoder = Encoder(output_channels=64, type_of_autoencoder=\"vae\")\n",
    "\n",
    "random_input_tensor = torch.randn(1, 1, 28, 28)\n",
    "output_tensor = encoder(random_input_tensor)\n",
    "\n",
    "\n",
    "del encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e534d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "f17e534d",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de375c0897a91a5a2154f5ce9386c72c",
     "grade": false,
     "grade_id": "cell-ef1c74bb6101690f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `q2b`: `Decoder` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451584ea",
   "metadata": {
    "deletable": false,
    "id": "451584ea",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0fe4eb084b81a03bade98cb0f850ce07",
     "grade": false,
     "grade_id": "cell-72c4cd996db5c348",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, input_channels: int, type_of_autoencoder: str):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        torch.manual_seed(42) \n",
    "        self.type_of_autoencoder = type_of_autoencoder\n",
    "\n",
    "        class ResidualBlock(torch.nn.Module):\n",
    "            def __init__(self, in_channels: int):\n",
    "                super(ResidualBlock, self).__init__()\n",
    "\n",
    "                torch.manual_seed(42)  \n",
    "                self.residual_block = torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "                    torch.nn.InstanceNorm2d(in_channels),\n",
    "                    torch.nn.LeakyReLU(negative_slope=0.1),\n",
    "                    torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "                    torch.nn.InstanceNorm2d(in_channels)\n",
    "                )\n",
    "\n",
    "            def forward(self, x):\n",
    "                return x + self.residual_block(x)  \n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(input_channels, 128, 3, stride=1, padding=1),\n",
    "            torch.nn.InstanceNorm2d(128),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            ResidualBlock(128),\n",
    "            torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            torch.nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            torch.nn.InstanceNorm2d(64),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            ResidualBlock(64),\n",
    "            torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            torch.nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            torch.nn.InstanceNorm2d(32),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            ResidualBlock(32),\n",
    "            torch.nn.Upsample(size=(28, 28), mode='bilinear', align_corners=False),\n",
    "            torch.nn.Conv2d(32, 1, 3, stride=1, padding=1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # num_params = sum(p.numel() for p in self.parameters())\n",
    "        # print(f\"Decoder parameters: {num_params}\")\n",
    "\n",
    "        # reference: https://datascience.stackexchange.com/questions/49709/what-is-the-best-architecture-for-auto-encoder-for-image-reconstruction\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b4533",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "d25b4533",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d81ee273dd30e560f3bf8c9382097fd",
     "grade": true,
     "grade_id": "cell-aba8bdf951c0b14d",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests for q2b\n",
    "\n",
    "decoder = Decoder(input_channels=64, type_of_autoencoder=\"vae\")\n",
    "\n",
    "random_input_tensor = torch.randn(1, 64, 4, 4)\n",
    "output_tensor = decoder(random_input_tensor)\n",
    "\n",
    "\n",
    "del decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6b592",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8da6b592",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "355eceafe54e9baa8373260d3a8f4d61",
     "grade": false,
     "grade_id": "cell-d67cb659437ac2d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# `q3`: AutoEncoder Model\n",
    "* `q3a`: `AutoEncoder` class: Implement a AutoEncoder that uses the Encoder and Decoder Modules implemented in `q2a` and `q2b`. Constraints:\n",
    "  1. The number of parameters in the AutoEncoder must be between 4,000 and 2,000,000 (both inclusive).\n",
    "  2. The input tensor will be of shape `[batch_size, 1, 28, 28]` that comes out of the DataLoader of the `FashionMNIST` dataset.\n",
    "  3. The output tensor must be of shape `[batch_size, 1, 28, 28]`. This tensor will be the reconstructed image of the input tensor.\n",
    "\n",
    "* `q3b`: Training the models: Implement the training loop for the AutoEncoder model. Constraints:\n",
    "  1. Use the `FashionMNIST` dataset implemented in `q1` to load the data.\n",
    "  2. Use the `AutoEncoder` model implemented in `q3a`.\n",
    "  3. You are free to choose any loss function, optimizer, and hyperparameters.\n",
    "  4. **You must**:\n",
    "     1. Book-keep the training and validation losses and SSIM scores for each epoch and use it to plot the training curves with the `AutoEncoder.save_training_report` method.\n",
    "     2. To calculate the SSIM score, you can use the `get_ssim` function provided below.\n",
    "     3. Save the model weights using `AutoEncoder.save_model_weights` method.\n",
    "\n",
    "\n",
    "`q3` Grading [Total: 1.5 points]:\n",
    "1. `q3a`: `AutoEncoder` class: `0.5` points if the code runs without any errors on hidden test cases, otherwise 0 points. No partial points for this question.\n",
    "2. `q3b`: Training the models: `1` points. You will be awarded points based on the SSIM score of the `AutoEncoder` model on a **hidden test set**. The grading will be as follows:\n",
    "   1. 0.8 or more: `1` point\n",
    "   2. 0.7 to 0.79: `0.8` points\n",
    "   3. 0.6 to 0.69: `0.6` points\n",
    "   4. 0.5 to 0.59: `0.4` points\n",
    "   5. 0.4 to 0.49: `0.2` points\n",
    "   6. Less than 0.4: `0` points\n",
    "\n",
    "\n",
    "You are provided with the following template. **Populate only the sections marked as `# YOUR CODE HERE`. Do not modify other parts of the template.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851bba2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "d851bba2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cdb87617aba0d4fb4b04e357611f17f3",
     "grade": false,
     "grade_id": "cell-3f5327832830d42f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `q3a`: `AutoEncoder` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4820e2",
   "metadata": {
    "deletable": false,
    "id": "7d4820e2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6295f3b83dd4783e08e7b66f2f7e03ea",
     "grade": false,
     "grade_id": "cell-c095dffe3237e062",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(output_channels=latent_dim, type_of_autoencoder=\"ae\")\n",
    "        self.decoder = Decoder(input_channels=latent_dim, type_of_autoencoder=\"ae\")\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \n",
    "        # print(f'Input shape is {input_tensor.shape}')\n",
    "        latent = self.encoder(input_tensor)\n",
    "        # print(f'Latent shape is {latent.shape}')\n",
    "        reconstructed = self.decoder(latent)\n",
    "        # print(f'Reconstructed shape is {reconstructed.shape}')\n",
    "\n",
    "        return reconstructed\n",
    "\n",
    "    def save_model_weights(self):\n",
    "        torch.save(self.state_dict(), \"auto_encoder.pth\")\n",
    "\n",
    "    def load_model_weights(self):\n",
    "        self.load_state_dict(torch.load(\"auto_encoder.pth\"))\n",
    "\n",
    "    def save_training_report(\n",
    "        self,\n",
    "        list_of_train_losses: List[float],\n",
    "        list_of_val_losses: List[float],\n",
    "        list_of_train_ssim_scores: List[float],\n",
    "        list_of_val_ssim_scores: List[float],\n",
    "    ):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Loss per Epoch\")\n",
    "        plt.plot(list_of_train_losses, label=\"Training\")\n",
    "        plt.plot(list_of_val_losses, label=\"Validation\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"SSIM per Epoch\")\n",
    "        plt.plot(list_of_train_ssim_scores, label=\"Training\")\n",
    "        plt.plot(list_of_val_ssim_scores, label=\"Validation\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"SSIM\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.suptitle(\"AutoEncoder Training Report\")\n",
    "\n",
    "        plt.savefig(\"auto_encoder.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9886dc4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e9886dc4",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70b93c299aaf9f281134acf8f90ec88b",
     "grade": true,
     "grade_id": "cell-a3e42ddd163c3984",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests for q3a\n",
    "\n",
    "autoencoder = AutoEncoder(latent_dim=64)\n",
    "\n",
    "random_input_tensor = torch.randn(1, 1, 28, 28)\n",
    "output_tensor = autoencoder(random_input_tensor)\n",
    "\n",
    "\n",
    "del autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f21c54",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "64f21c54",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c7b5a2136e3dcd1216b7e542d3ff16d",
     "grade": false,
     "grade_id": "cell-65078b6416cca373",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `q3b`: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63461f1a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "63461f1a",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ceb34aa91fc9468cb1272d8233d2f8c",
     "grade": false,
     "grade_id": "cell-38c2e638d5b531c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_ssim(\n",
    "    list_of_predicted_images: List[NumpyArray], list_of_true_images: List[NumpyArray]\n",
    ") -> List[float]:\n",
    "    ssim_values = []\n",
    "    for predicted_image, true_image in zip(\n",
    "        list_of_predicted_images, list_of_true_images\n",
    "    ):\n",
    "        assert predicted_image.shape == (\n",
    "            28,\n",
    "            28,\n",
    "        ), f\"Expected image of shape (28, 28) but got {predicted_image.shape}\"\n",
    "        assert true_image.shape == (\n",
    "            28,\n",
    "            28,\n",
    "        ), f\"Expected image of shape (28, 28) but got {true_image.shape}\"\n",
    "        ssim_values.append(\n",
    "            ssim(\n",
    "                predicted_image,\n",
    "                true_image,\n",
    "                data_range=true_image.max() - true_image.min(),\n",
    "            )\n",
    "        )\n",
    "    return sum(ssim_values) / len(ssim_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f032eb85",
   "metadata": {
    "deletable": false,
    "id": "f032eb85",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f727e24e154cd61dba5dcfda0712500",
     "grade": false,
     "grade_id": "cell-7a9e6fc60429fdec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use this cell to:\n",
    "# 1. Train the AutoEncoder model while bookkeeping the training and validation losses and SSIM scores for each epoch\n",
    "# 2. Save the model weights using AutoEncoder.save_model_weights method\n",
    "# 3. Save the training report using AutoEncoder.save_training_report method\n",
    "\n",
    "\n",
    "def augment_fashion_mnist_dataset(dataset: FashionMNISTDataset) -> FashionMNISTDataset:\n",
    "    \"\"\"\n",
    "    Augments the dataset by adding horizontally flipped clean images as augmented versions.\n",
    "    This doubles the dataset size to 120K samples.\n",
    "    \"\"\"\n",
    "    flip_transform = torchvision.transforms.RandomHorizontalFlip(p=1.0)\n",
    "    \n",
    "    new_augmented_images = []\n",
    "    new_clean_images = []\n",
    "    new_labels = []\n",
    "    \n",
    "    for clean_img, aug_img, label in zip(dataset.clean_images_images, dataset.augmented_images_images, dataset.labels):\n",
    "        flipped_clean_img = flip_transform(clean_img)\n",
    "        \n",
    "        # Original pair\n",
    "        new_clean_images.append(clean_img)\n",
    "        new_augmented_images.append(aug_img)\n",
    "        new_labels.append(label)\n",
    "        \n",
    "        # Flipped pair\n",
    "        new_clean_images.append(clean_img)\n",
    "        new_augmented_images.append(flipped_clean_img)\n",
    "        new_labels.append(label)\n",
    "    \n",
    "    augmented_dataset = FashionMNISTDataset.__new__(FashionMNISTDataset)\n",
    "    augmented_dataset.clean_images_images = new_clean_images\n",
    "    augmented_dataset.augmented_images_images = new_augmented_images\n",
    "    augmented_dataset.labels = new_labels\n",
    "    \n",
    "    return augmented_dataset\n",
    "\n",
    "\n",
    "def train_autoencoder(model, train_dataloader, test_dataloader, epochs=50, learning_rate=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # criterion = torch.nn.MSELoss()\n",
    "    criterion = torch.nn.L1Loss()\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_ssim_scores, val_ssim_scores = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss, total_train_ssim = 0, 0\n",
    "\n",
    "        for aug_img, clean_img, _ in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            aug_img, clean_img = aug_img.to(device), clean_img.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(aug_img)\n",
    "            loss = criterion(output, clean_img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ssim_score = get_ssim(\n",
    "                output.detach().cpu().numpy().squeeze(), clean_img.cpu().numpy().squeeze()\n",
    "            )\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            total_train_ssim += ssim_score\n",
    "\n",
    "        train_losses.append(total_train_loss/len(train_dataloader))\n",
    "        train_ssim_scores.append(total_train_ssim/len(train_dataloader))\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss, total_val_ssim = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for aug_img, clean_img, _ in test_dataloader:\n",
    "                aug_img, clean_img = aug_img.to(device), clean_img.to(device)\n",
    "\n",
    "                output = model(aug_img)\n",
    "                # print(output.shape)\n",
    "                # print(clean_img.shape)\n",
    "                loss = criterion(output, clean_img)\n",
    "\n",
    "                ssim_score = get_ssim(\n",
    "                    output.cpu().numpy().squeeze(), clean_img.cpu().numpy().squeeze()\n",
    "                )\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "                total_val_ssim += ssim_score\n",
    "\n",
    "        val_losses.append(total_val_loss/len(test_dataloader))\n",
    "        val_ssim_scores.append(total_val_ssim/len(test_dataloader))\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] --> Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Train SSIM: {train_ssim_scores[-1]:.4f}, Val SSIM: {val_ssim_scores[-1]:.4f}\")\n",
    "\n",
    "    model.save_model_weights()\n",
    "    model.save_training_report(train_losses, val_losses, train_ssim_scores, val_ssim_scores)\n",
    "    return model\n",
    "\n",
    "\n",
    "# tests for q1\n",
    "\n",
    "path_to_train_images_aug_dir = str(PATH_TO_TRAIN_DATA_DIR + \"/aug\")\n",
    "path_to_train_images_clean_dir = str(PATH_TO_TRAIN_DATA_DIR + \"/clean\")\n",
    "\n",
    "path_to_test_images_aug_dir = str(PATH_TO_TEST_DATA_DIR + \"/aug\")\n",
    "path_to_test_images_clean_dir = str(PATH_TO_TEST_DATA_DIR + \"/clean\")\n",
    "\n",
    "\n",
    "# commented out because of the long time it takes to run - already ran and saved the data\n",
    "\n",
    "fashion_mnist_dataset_train = FashionMNISTDataset(\n",
    "    path_to_augmented_images_dir=path_to_train_images_aug_dir,\n",
    "    path_to_clean_images_dir=path_to_train_images_clean_dir,\n",
    ")\n",
    "\n",
    "fashion_mnist_dataset_test = FashionMNISTDataset(\n",
    "    path_to_augmented_images_dir=path_to_test_images_aug_dir,\n",
    "    path_to_clean_images_dir=path_to_test_images_clean_dir,\n",
    ")\n",
    "\n",
    "augmented_fashion_mnist_dataset_train = augment_fashion_mnist_dataset(fashion_mnist_dataset_train)\n",
    "# print(len(augmented_fashion_mnist_dataset_train))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=augmented_fashion_mnist_dataset_train, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=fashion_mnist_dataset_test, batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "autoencoder = AutoEncoder(latent_dim=256)\n",
    "\n",
    "autoencoder = train_autoencoder(autoencoder, train_dataloader, test_dataloader, epochs=50, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253ac7b-3a7b-4cc9-a275-f07641d0053f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8253ac7b-3a7b-4cc9-a275-f07641d0053f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a42141696fc0c895626537a50aba0e1",
     "grade": true,
     "grade_id": "cell-a2e4e8700fdcfb80",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests for q3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8ac3e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b6e8ac3e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "382a5e2833d97007f5c3e1b07000a523",
     "grade": false,
     "grade_id": "cell-8f7b007d1bcbf8e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# `q4`: Variational AutoEncoder Model\n",
    "* `q4a`: `VariationalAutoEncoder` class: Implement a VariationalAutoEncoder that uses the Encoder and Decoder Modules implemented in `q2a` and `q2b`. Constraints:\n",
    "  1. The number of parameters in the VariationalAutoEncoder must be between 4,000 and 2,000,000 (both inclusive).\n",
    "  2. The input tensor will be of shape `[batch_size, 1, 28, 28]` that comes out of the DataLoader of the `FashionMNIST` dataset.\n",
    "  3. The output tensor must be of shape `[batch_size, 1, 28, 28]`. This tensor will be the reconstructed image of the input tensor.\n",
    "\n",
    "* `q4b`: Training the models: Implement the training loop for the VariationalAutoEncoder model. Constraints:\n",
    "  1. Use the `FashionMNIST` dataset implemented in `q1` to load the data.\n",
    "  2. Use the `VariationalAutoEncoder` model implemented in `q4a`.\n",
    "  3. You are free to choose any loss function, optimizer, and hyperparameters.\n",
    "  4. **You must**:\n",
    "     1. Book-keep the training and validation losses and SSIM scores for each epoch and use it to plot the training curves with the `VariationalAutoEncoder.save_training_report` method.\n",
    "     2. To calculate the SSIM score, you can use the `get_ssim` function provided below.\n",
    "     3. Save the model weights using `VariationalAutoEncoder.save_model_weights` method.\n",
    "\n",
    "\n",
    "`q4` Grading [Total: 1.5 points]:\n",
    "1. `q4a`: `VariationalAutoEncoder` class: `0.5` points if the code runs without any errors on hidden test cases, otherwise 0 points. No partial points for this question.\n",
    "2. `q4b`: Training the models: `1` points. You will be awarded points based on the SSIM score of the `VariationalAutoEncoder` model on a **hidden test set**. The grading will be as follows:\n",
    "   1. 0.8 or more: `1` point\n",
    "   2. 0.7 or more: `0.8` points\n",
    "   3. 0.6 or more: `0.6` points\n",
    "   4. 0.5 or more: `0.4` points\n",
    "   5. 0.4 or more: `0.2` points\n",
    "   6. Less than 0.4: `0` points\n",
    "\n",
    "You are provided with the following template. **Populate only the sections marked as `# YOUR CODE HERE`. Do not modify other parts of the template.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67530b04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "67530b04",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66752f0b784a02c127ff8a044608eb65",
     "grade": false,
     "grade_id": "cell-897af261c2d06d30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `q4a`: `VariationalAutoEncoder` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0f31c",
   "metadata": {
    "deletable": false,
    "id": "9ef0f31c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bce3eede85dba7a8fb5be994467d265c",
     "grade": false,
     "grade_id": "cell-50018b56d6158ef5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(output_channels=latent_dim * 2, type_of_autoencoder=\"vae\")\n",
    "        self.decoder = Decoder(input_channels=latent_dim, type_of_autoencoder=\"vae\")\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.distributions.Laplace(mu, std).rsample()\n",
    "        return eps\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \n",
    "        latent = self.encoder(input_tensor)  \n",
    "        mu, log_var = torch.chunk(latent, 2, dim=1)\n",
    "        z = self.reparameterize(mu, log_var)  \n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mu, log_var\n",
    "\n",
    "    def loss_function(self, predicted_images, gt_images, mu, log_var):\n",
    "        criterion = torch.nn.L1Loss(reduction='mean')\n",
    "        l1_loss = criterion(predicted_images, gt_images)\n",
    "\n",
    "        p = torch.distributions.Laplace(mu, torch.exp(0.5 * log_var))\n",
    "        q = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(log_var))\n",
    "        kl_divergence = torch.distributions.kl.kl_divergence(p, q).mean()\n",
    "        js_divergence = 0.5 * (kl_divergence + torch.distributions.kl.kl_divergence(q, p).mean())\n",
    "\n",
    "        return l1_loss + js_divergence\n",
    "\n",
    "    def save_model_weights(self):\n",
    "        torch.save(self.state_dict(), \"variational_auto_encoder.pth\")\n",
    "\n",
    "    def load_model_weights(self):\n",
    "        self.load_state_dict(torch.load(\"variational_auto_encoder.pth\"))\n",
    "\n",
    "    def save_training_report(\n",
    "        self,\n",
    "        list_of_train_losses: List[float],\n",
    "        list_of_val_losses: List[float],\n",
    "        list_of_train_ssim_scores: List[float],\n",
    "        list_of_val_ssim_scores: List[float],\n",
    "    ):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Loss per Epoch\")\n",
    "        plt.plot(list_of_train_losses, label=\"Training\")\n",
    "        plt.plot(list_of_val_losses, label=\"Validation\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"SSIM per Epoch\")\n",
    "        plt.plot(list_of_train_ssim_scores, label=\"Training\")\n",
    "        plt.plot(list_of_val_ssim_scores, label=\"Validation\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"SSIM\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.suptitle(\"VariationalAutoEncoder Training Report\")\n",
    "\n",
    "        plt.savefig(\"variational_auto_encoder.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19e4e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8e19e4e3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9743b03fdaabcc90c9b291e4bc45b4e2",
     "grade": true,
     "grade_id": "cell-fb81ba17291dc3b0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests for q4a\n",
    "\n",
    "variational_autoencoder = VariationalAutoEncoder(latent_dim=64)\n",
    "\n",
    "random_input_tensor = torch.randn(1, 1, 28, 28)\n",
    "output = variational_autoencoder(random_input_tensor)\n",
    "\n",
    "\n",
    "del variational_autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56ef92",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7c56ef92",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "26dc99239a593c1549c64375c7d685ab",
     "grade": false,
     "grade_id": "cell-9d6349d66274c9ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `q4b`: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f88a4c",
   "metadata": {
    "deletable": false,
    "id": "15f88a4c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c621752f2b40a4b066d930c0b1ff621",
     "grade": false,
     "grade_id": "cell-0aea62ae6e9aaf70",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use this cell to:\n",
    "# 1. Train the VariationalAutoEncoder model while bookkeeping the training and validation losses and SSIM scores for each epoch\n",
    "# 2. Save the model weights using VariationalAutoEncoder.save_model_weights method\n",
    "# 3. Save the training report using VariationalAutoEncoder.save_training_report method\n",
    "\n",
    "\n",
    "def augment_fashion_mnist_dataset(dataset: FashionMNISTDataset) -> FashionMNISTDataset:\n",
    "    \"\"\"\n",
    "    Augments the dataset by adding horizontally flipped clean images as augmented versions.\n",
    "    This doubles the dataset size to 120K samples.\n",
    "    \"\"\"\n",
    "    flip_transform = torchvision.transforms.RandomHorizontalFlip(p=1.0)\n",
    "\n",
    "    new_augmented_images = []\n",
    "    new_clean_images = []\n",
    "    new_labels = []\n",
    "\n",
    "    for clean_img, aug_img, label in zip(dataset.clean_images_images, dataset.augmented_images_images, dataset.labels):\n",
    "        flipped_clean_img = flip_transform(clean_img)\n",
    "\n",
    "        # Original pair\n",
    "        new_clean_images.append(clean_img)\n",
    "        new_augmented_images.append(aug_img)\n",
    "        new_labels.append(label)\n",
    "\n",
    "        # Flipped pair\n",
    "        new_clean_images.append(clean_img)\n",
    "        new_augmented_images.append(flipped_clean_img)\n",
    "        new_labels.append(label)\n",
    "\n",
    "    augmented_dataset = FashionMNISTDataset.__new__(FashionMNISTDataset)\n",
    "    augmented_dataset.clean_images_images = new_clean_images\n",
    "    augmented_dataset.augmented_images_images = new_augmented_images\n",
    "    augmented_dataset.labels = new_labels\n",
    "\n",
    "    return augmented_dataset\n",
    "\n",
    "def train_vae(model, train_dataloader, test_dataloader, epochs=50, learning_rate=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_ssim_scores, val_ssim_scores = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss, total_train_ssim = 0, 0\n",
    "\n",
    "        for aug_img, clean_img, _ in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            aug_img, clean_img = aug_img.to(device), clean_img.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_images, mu, log_var = model(aug_img)\n",
    "\n",
    "            loss = model.loss_function(reconstructed_images, clean_img, mu, log_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ssim_score = get_ssim(\n",
    "                reconstructed_images.detach().cpu().numpy().squeeze(),\n",
    "                clean_img.cpu().numpy().squeeze(),\n",
    "            )\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            total_train_ssim += ssim_score\n",
    "\n",
    "        train_losses.append(total_train_loss / len(train_dataloader))\n",
    "        train_ssim_scores.append(total_train_ssim / len(train_dataloader))\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss, total_val_ssim = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for aug_img, clean_img, _ in test_dataloader:\n",
    "                aug_img, clean_img = aug_img.to(device), clean_img.to(device)\n",
    "\n",
    "                reconstructed_images, mu, log_var = model(aug_img)\n",
    "\n",
    "                loss = model.loss_function(reconstructed_images, clean_img, mu, log_var)\n",
    "\n",
    "                ssim_score = get_ssim(\n",
    "                    reconstructed_images.cpu().numpy().squeeze(),\n",
    "                    clean_img.cpu().numpy().squeeze(),\n",
    "                )\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "                total_val_ssim += ssim_score\n",
    "\n",
    "        val_losses.append(total_val_loss / len(test_dataloader))\n",
    "        val_ssim_scores.append(total_val_ssim / len(test_dataloader))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{epochs}] --> \"\n",
    "            f\"Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, \"\n",
    "            f\"Train SSIM: {train_ssim_scores[-1]:.4f}, Val SSIM: {val_ssim_scores[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "    model.save_model_weights()\n",
    "    model.save_training_report(train_losses, val_losses, train_ssim_scores, val_ssim_scores)\n",
    "    return model\n",
    "\n",
    "\n",
    "# tests for q1\n",
    "\n",
    "path_to_train_images_aug_dir = str(PATH_TO_TRAIN_DATA_DIR + \"/aug\")\n",
    "path_to_train_images_clean_dir = str(PATH_TO_TRAIN_DATA_DIR + \"/clean\")\n",
    "\n",
    "path_to_test_images_aug_dir = str(PATH_TO_TEST_DATA_DIR + \"/aug\")\n",
    "path_to_test_images_clean_dir = str(PATH_TO_TEST_DATA_DIR + \"/clean\")\n",
    "\n",
    "\n",
    "# commented out because of the long time it takes to run - already ran and saved the data\n",
    "\n",
    "fashion_mnist_dataset_train = FashionMNISTDataset(\n",
    "    path_to_augmented_images_dir=path_to_train_images_aug_dir,\n",
    "    path_to_clean_images_dir=path_to_train_images_clean_dir,\n",
    ")\n",
    "\n",
    "fashion_mnist_dataset_test = FashionMNISTDataset(\n",
    "    path_to_augmented_images_dir=path_to_test_images_aug_dir,\n",
    "    path_to_clean_images_dir=path_to_test_images_clean_dir,\n",
    ")\n",
    "\n",
    "augmented_fashion_mnist_dataset_train = augment_fashion_mnist_dataset(fashion_mnist_dataset_train)\n",
    "# print(len(augmented_fashion_mnist_dataset_train))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=augmented_fashion_mnist_dataset_train, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=fashion_mnist_dataset_test, batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "variational_autoencoder = VariationalAutoEncoder(latent_dim=128)\n",
    "\n",
    "variational_autoencoder = train_vae(variational_autoencoder, train_dataloader, test_dataloader, epochs=50, learning_rate=5e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e756b79e-a3bc-4fed-868d-9229608db4c1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e756b79e-a3bc-4fed-868d-9229608db4c1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc7ca536b2223b235453d4c5de1a9f02",
     "grade": true,
     "grade_id": "cell-7784e404fa72538a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests for q4b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2cf5a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2a2cf5a7",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc470904174ea8ba369214a879982556",
     "grade": false,
     "grade_id": "cell-8ff254023bae8425",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# `q5`: [BONUS] Conditional Variational AutoEncoder Model\n",
    "* `q5a`: `ConditionalVariationalAutoEncoder` class: Implement a ConditionalVariationalAutoEncoder that uses the Encoder and Decoder Modules implemented in `q2a` and `q2b`. Constraints:\n",
    "  1. The number of parameters in the ConditionalVariationalAutoEncoder must be between 4,000 and 2,000,000 (both inclusive).\n",
    "  2. The input tensor will be of shape `[batch_size, 1, 28, 28]` that comes out of the DataLoader of the `FashionMNIST` dataset.\n",
    "  3. The output tensor must be of shape `[batch_size, 1, 28, 28]`. This tensor will be the reconstructed image of the input tensor.\n",
    "\n",
    "* `q5b`: Training the models: Implement the training loop for the ConditionalVariationalAutoEncoder model. Constraints:\n",
    "  1. Use the `FashionMNIST` dataset implemented in `q1` to load the data.\n",
    "  2. Use the `ConditionalVariationalAutoEncoder` model implemented in `q5a`.\n",
    "  3. You are free to choose any loss function, optimizer, and hyperparameters.\n",
    "  4. **You must**:\n",
    "     1. Book-keep the training and validation losses and SSIM scores for each epoch and use it to plot the training curves with the `ConditionalVariationalAutoEncoder.save_training_report` method.\n",
    "     2. To calculate the SSIM score, you can use the `get_ssim` function provided below.\n",
    "     3. Save the model weights using `ConditionalVariationalAutoEncoder.save_model_weights` method.\n",
    "\n",
    "\n",
    "`q5` Grading [Total: 1 point]:\n",
    "1. `q5b`: Training the models: `1` point. You will be awarded points based on the SSIM score of the `ConditionalVariationalAutoEncoder` model on a **hidden test set**. The grading will be as follows:\n",
    "   1. 0.8 or more: `1` point\n",
    "   2. 0.7 to 0.79: `0.5` points\n",
    "   3. less than 0.7: `0` points\n",
    "\n",
    "You are provided with the following template. **Populate only the sections marked as `# YOUR CODE HERE`. Do not modify other parts of the template.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd3d62",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6bfd3d62",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8027ac7c9c037cb063eb577db3b98dcb",
     "grade": false,
     "grade_id": "cell-219f459034a85820",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `q5a`: [BONUS]`ConditionalVariationalAutoEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d7b335",
   "metadata": {
    "deletable": false,
    "id": "29d7b335",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46202c4d0e0385e99644013adea0cc20",
     "grade": false,
     "grade_id": "cell-897bbc51499b7fd5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ConditionalVariationalAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, latent_dim: int, condition_dim: int):\n",
    "        super(ConditionalVariationalAutoEncoder, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        self.encoder = Encoder(output_channels=latent_dim * 2, type_of_autoencoder=\"cvae\")\n",
    "        self.decoder = Decoder(input_channels=latent_dim, type_of_autoencoder=\"cvae\")\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.distributions.Laplace(mu, std).rsample()\n",
    "        return eps\n",
    "\n",
    "    def forward(self, input_tensor, condition_tensor):\n",
    "\n",
    "        condition_tensor = condition_tensor.view(condition_tensor.size(0), -1, 1, 1)\n",
    "        # print(f'Condition shape is {condition_tensor.shape}')\n",
    "        # conditioned_input = torch.cat((input_tensor, condition_tensor), dim=1)\n",
    "        \n",
    "        latent = self.encoder(input_tensor)\n",
    "        mu, log_var = torch.chunk(latent, 2, dim=1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "\n",
    "        _z = z.view(-1, self.latent_dim, 1, 1)\n",
    "        # z = torch.cat((z, condition_tensor), dim=1)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mu, log_var\n",
    "\n",
    "    def loss_function(self, predicted_images, gt_images, mu, log_var):\n",
    "        \n",
    "        criterion = torch.nn.L1Loss(reduction='mean')\n",
    "        l1_loss = criterion(predicted_images, gt_images)\n",
    "\n",
    "        p = torch.distributions.Laplace(mu, torch.exp(0.5 * log_var))\n",
    "        q = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(log_var))\n",
    "        kl_divergence = torch.distributions.kl.kl_divergence(p, q).mean()\n",
    "        js_divergence = 0.5 * (kl_divergence + torch.distributions.kl.kl_divergence(q, p).mean())\n",
    "\n",
    "        return l1_loss + js_divergence\n",
    "\n",
    "    def save_model_weights(self):\n",
    "        torch.save(self.state_dict(), \"conditional_variational_auto_encoder.pth\")\n",
    "\n",
    "    def load_model_weights(self):\n",
    "        self.load_state_dict(torch.load(\"conditional_variational_auto_encoder.pth\"))\n",
    "\n",
    "    def save_training_report(\n",
    "        self,\n",
    "        list_of_train_losses: List[float],\n",
    "        list_of_val_losses: List[float],\n",
    "        list_of_train_ssim_scores: List[float],\n",
    "        list_of_val_ssim_scores: List[float],\n",
    "    ):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Loss per Epoch\")\n",
    "        plt.plot(list_of_train_losses, label=\"Training\")\n",
    "        plt.plot(list_of_val_losses, label=\"Validation\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"SSIM per Epoch\")\n",
    "        plt.plot(list_of_train_ssim_scores, label=\"Training\")\n",
    "        plt.plot(list_of_val_ssim_scores, label=\"Validation\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"SSIM\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.suptitle(\"ConditionalVariationalAutoEncoder Training Report\")\n",
    "\n",
    "        plt.savefig(\"conditional_variational_auto_encoder.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b58dc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "047b58dc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1a03cab93a7876fc0502e6503bd4e6f",
     "grade": true,
     "grade_id": "cell-53fef8fb01242647",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests for q5a\n",
    "\n",
    "conditional_variational_autoencoder = ConditionalVariationalAutoEncoder(\n",
    "    latent_dim=64, condition_dim=10\n",
    ")\n",
    "\n",
    "random_input_tensor = torch.randn(1, 1, 28, 28)\n",
    "random_condition_tensor = torch.randn(1, 10)\n",
    "output = conditional_variational_autoencoder(\n",
    "    random_input_tensor, random_condition_tensor\n",
    ")\n",
    "\n",
    "\n",
    "del conditional_variational_autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479811ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "479811ee",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15c443b0bed37da59019887932b70ac9",
     "grade": false,
     "grade_id": "cell-5f32276d86af0895",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `q5b`: [BONUS] Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e38dc58",
   "metadata": {
    "deletable": false,
    "id": "3e38dc58",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3beaab2f82c0121782c2bd7506375a93",
     "grade": false,
     "grade_id": "cell-93d3753839989066",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use this cell to:\n",
    "# 1. Train the ConditionalVariationalAutoEncoder model while bookkeeping the training and validation losses and SSIM scores for each epoch\n",
    "# 2. Save the model weights using ConditionalVariationalAutoEncoder.save_model_weights method\n",
    "# 3. Save the training report using ConditionalVariationalAutoEncoder.save_training_report method\n",
    "\n",
    "\n",
    "def augment_fashion_mnist_dataset(dataset: FashionMNISTDataset) -> FashionMNISTDataset:\n",
    "    \"\"\"\n",
    "    Augments the dataset by adding horizontally flipped clean images as augmented versions.\n",
    "    This doubles the dataset size to 120K samples.\n",
    "    \"\"\"\n",
    "    flip_transform = torchvision.transforms.RandomHorizontalFlip(p=1.0)\n",
    "\n",
    "    new_augmented_images = []\n",
    "    new_clean_images = []\n",
    "    new_labels = []\n",
    "\n",
    "    for clean_img, aug_img, label in zip(dataset.clean_images_images, dataset.augmented_images_images, dataset.labels):\n",
    "        flipped_clean_img = flip_transform(clean_img)\n",
    "\n",
    "        # Original pair\n",
    "        new_clean_images.append(clean_img)\n",
    "        new_augmented_images.append(aug_img)\n",
    "        new_labels.append(label)\n",
    "\n",
    "        # Flipped pair\n",
    "        new_clean_images.append(clean_img)\n",
    "        new_augmented_images.append(flipped_clean_img)\n",
    "        new_labels.append(label)\n",
    "\n",
    "    augmented_dataset = FashionMNISTDataset.__new__(FashionMNISTDataset)\n",
    "    augmented_dataset.clean_images_images = new_clean_images\n",
    "    augmented_dataset.augmented_images_images = new_augmented_images\n",
    "    augmented_dataset.labels = new_labels\n",
    "\n",
    "    return augmented_dataset\n",
    "\n",
    "def train_cvae(model, train_dataloader, test_dataloader, epochs=50, learning_rate=1e-3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_ssim_scores, val_ssim_scores = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss, total_train_ssim = 0, 0\n",
    "\n",
    "        for aug_img, clean_img, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            aug_img, clean_img, labels = aug_img.to(device), clean_img.to(device), labels.to(device)\n",
    "\n",
    "            condition_tensor = torch.nn.functional.one_hot(labels, num_classes=model.condition_dim).float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_images, mu, log_var = model(aug_img, condition_tensor)\n",
    "\n",
    "            loss = model.loss_function(reconstructed_images, clean_img, mu, log_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ssim_score = get_ssim(\n",
    "                reconstructed_images.detach().cpu().numpy().squeeze(),\n",
    "                clean_img.cpu().numpy().squeeze(),\n",
    "            )\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            total_train_ssim += ssim_score\n",
    "\n",
    "        train_losses.append(total_train_loss / len(train_dataloader))\n",
    "        train_ssim_scores.append(total_train_ssim / len(train_dataloader))\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss, total_val_ssim = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for aug_img, clean_img, labels in test_dataloader:\n",
    "                aug_img, clean_img, labels = aug_img.to(device), clean_img.to(device), labels.to(device)\n",
    "\n",
    "                condition_tensor = torch.nn.functional.one_hot(labels, num_classes=model.condition_dim).float().to(device)\n",
    "\n",
    "                reconstructed_images, mu, log_var = model(aug_img, condition_tensor)\n",
    "\n",
    "                loss = model.loss_function(reconstructed_images, clean_img, mu, log_var)\n",
    "\n",
    "                ssim_score = get_ssim(\n",
    "                    reconstructed_images.cpu().numpy().squeeze(),\n",
    "                    clean_img.cpu().numpy().squeeze(),\n",
    "                )\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "                total_val_ssim += ssim_score\n",
    "\n",
    "        val_losses.append(total_val_loss / len(test_dataloader))\n",
    "        val_ssim_scores.append(total_val_ssim / len(test_dataloader))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{epochs}] --> \"\n",
    "            f\"Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, \"\n",
    "            f\"Train SSIM: {train_ssim_scores[-1]:.4f}, Val SSIM: {val_ssim_scores[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "    model.save_model_weights()\n",
    "    model.save_training_report(train_losses, val_losses, train_ssim_scores, val_ssim_scores)\n",
    "    return model\n",
    "\n",
    "\n",
    "# tests for q1\n",
    "\n",
    "path_to_train_images_aug_dir = str(PATH_TO_TRAIN_DATA_DIR + \"/aug\")\n",
    "path_to_train_images_clean_dir = str(PATH_TO_TRAIN_DATA_DIR + \"/clean\")\n",
    "\n",
    "path_to_test_images_aug_dir = str(PATH_TO_TEST_DATA_DIR + \"/aug\")\n",
    "path_to_test_images_clean_dir = str(PATH_TO_TEST_DATA_DIR + \"/clean\")\n",
    "\n",
    "\n",
    "# commented out because of the long time it takes to run - already ran and saved the data\n",
    "\n",
    "fashion_mnist_dataset_train = FashionMNISTDataset(\n",
    "    path_to_augmented_images_dir=path_to_train_images_aug_dir,\n",
    "    path_to_clean_images_dir=path_to_train_images_clean_dir,\n",
    ")\n",
    "\n",
    "fashion_mnist_dataset_test = FashionMNISTDataset(\n",
    "    path_to_augmented_images_dir=path_to_test_images_aug_dir,\n",
    "    path_to_clean_images_dir=path_to_test_images_clean_dir,\n",
    ")\n",
    "\n",
    "augmented_fashion_mnist_dataset_train = augment_fashion_mnist_dataset(fashion_mnist_dataset_train)\n",
    "# print(len(augmented_fashion_mnist_dataset_train))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=augmented_fashion_mnist_dataset_train, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=fashion_mnist_dataset_test, batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "conditional_variational_autoencoder = ConditionalVariationalAutoEncoder(latent_dim=128, condition_dim=10)\n",
    "\n",
    "conditional_variational_autoencoder = train_cvae(conditional_variational_autoencoder, train_dataloader, test_dataloader, epochs=50, learning_rate=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6adadf-85a4-4bc3-b515-16e91bbe5193",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5b6adadf-85a4-4bc3-b515-16e91bbe5193",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4644e975dc1db68368a0a14613d8c03b",
     "grade": true,
     "grade_id": "cell-13c675af9381345f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests for q5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d12e1-5bdc-40ad-87c9-135ce0c553ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "558d12e1-5bdc-40ad-87c9-135ce0c553ae",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5a387ee031cf135774640a6a5c2c535",
     "grade": true,
     "grade_id": "cell-1b8718af75c8a798",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# tests for q3b, q4b, q5b\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
